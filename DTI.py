# -*- coding: utf-8 -*-
"""data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OXgbZO2D0uYH8LKZ8l_7b3N4U-JQAYLe
"""



#!/usr/bin/env python
#_*_coding:utf-8_*_

import re, os, sys

def readFasta(file):
	print("Current Directory", os.getcwd())
	if os.path.exists(file) == False:
		print('Error: "' + file + '" does not exist.')
		sys.exit(1)

	with open(file) as f:
		records = f.read()

	if re.search('>', records) == None:
		print('The input file seems not in fasta format.')
		sys.exit(1)

	records = records.split('>')[1:]
	myFasta = []
	for fasta in records:
		array = fasta.split('\n')
		name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())
		myFasta.append([name, sequence])
	return myFasta

import re
from collections import Counter
import numpy as np
import pandas as pd

def AAC(fastas, **kw):
	AA = kw['order'] if kw['order'] != None else 'ACDEFGHIKLMNPQRSTVWY'
	#AA = 'ARNDCQEGHILKMFPSTWYV'
	encodings = []
	header = ['#']
	for i in AA:
		header.append(i)
	encodings.append(header)

	for i in fastas:
		name, sequence = i[0], re.sub('-', '', i[1])
		count = Counter(sequence)
		for key in count:
			count[key] = count[key]/len(sequence)
		code = [name]
		for aa in AA:
			code.append(count[aa])
		encodings.append(code)
	return encodings

#kw={'path': r"Enzyme.txt",'order': 'ACDEFGHIKLMNPQRSTVWY'}
#kw={'path': r"GPCR.txt", 'order': 'ACDEFGHIKLMNPQRSTVWY'}
#kw={'path': r"Ion channel.txt", 'order': 'ACDEFGHIKLMNPQRSTVWY'}
kw= {'path': r"Nuclear receptor.txt", 'order': 'ACDEFGHIKLMNPQRSTVWY'}

#fastas1 = readFasta.readFasta(r"Enzyme.txt")
#fastas1 = readFasta.readFasta(r"GPCR.txt")
#fastas1 = readFasta.readFasta(r"Ion channel.txt")
fastas1 = readFasta(r"Nuclear receptor.txt")

result = AAC(fastas1, **kw)

data1 = np.matrix(result[1:])[:, 1:]
data_aac = pd.DataFrame(data=data1)

#data_aac.to_csv('EN_aac.csv')
#data_aac.to_csv('GPCR_aac.csv')
#data_aac.to_csv('IC_aac.csv')
data_aac.to_csv('NR_aac.csv')

import numpy as np
import pandas as pd
from sklearn.svm import OneClassSVM

from sklearn.model_selection import train_test_split

from sklearn import metrics

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV

import warnings
warnings.filterwarnings('ignore')

def One_SVM_US(df):

    count_label_0 = sum(df['label'] == 0)
    count_label_1 = sum(df['label'] == 1)

    # **Splitting samples of label(0) **
    df_label_0 = df.loc[df['label'] == 0]
    df_label_1 = df.loc[df['label'] == 1]

    df_X = df_label_0.copy()
    df_X.drop(['drug_no', 'protein_no', 'label'], axis=1, inplace=True)

    print(df_X.shape[0])

    svm = OneClassSVM(kernel='rbf', gamma=1.0 / df_X.shape[0], tol=0.001, nu=0.5, shrinking=True, cache_size=80)
    svm = svm.fit(df_X.values)
    print(svm)

    # decision_function it says that its the distance between the hyperplane and the test instance
    # flatten() function we can flatten a matrix to one dimension in python.
    scores = svm.decision_function(df_X.values).flatten()
    maxvalue = np.max(scores)
    print(maxvalue)

    scores = maxvalue - scores
    print(scores)

    scores = pd.DataFrame(scores)
    scores.rename(columns={0: 'score'}, inplace=True)

    samples = [i for i in range(count_label_0)]
    samples = pd.DataFrame(samples)
    samples.rename(columns={0: 'sample'}, inplace=True)

    df_scores = pd.concat([samples, scores], axis=1, sort=False)
    df_scores_reordered = df_scores.sort_values('score', ascending=True)

    df_low_score = df_scores_reordered.head(count_label_1)
    print(df_low_score.head())
    print(len(df_low_score))

    index_list = df_low_score['sample'].tolist()
    print(index_list)

    df_final_label_0 = df_label_0.iloc[index_list, :]
    print(df_final_label_0.head())

    # append method
    df_final = df_final_label_0._append(df_label_1)
    df_final.reset_index(inplace=True)
    df_final.drop(['index'], axis=1, inplace=True)
    print(df_final.head())
    print(len(df_final))

    return df_final

# **** Function for removing features with correlation higher than 0.8 ****
def remove_correlated_features(df):
    correlated_features = set()
    correlation_matrix = df.corr()
    for i in range(len(correlation_matrix.columns)):
        for j in range(i):
            if abs(correlation_matrix.iloc[i, j]) > 0.8:
                colname = correlation_matrix.columns[i]
                correlated_features.add(colname)
    return correlated_features

def FFS_RF(df_group):
    y = df_group['label']
    df_group.drop(['label', 'protein_no', 'drug_no'], axis=1, inplace=True)
    print(df_group.head())

    correlated_features = remove_correlated_features(df_group)
    df_group.drop(labels=correlated_features, axis=1, inplace=True)
    print(df_group.head())
    X = df_group

    X_train, X_test, y_train, y_test = train_test_split(df_group, y, test_size=0.2, random_state=42)

    n_samples, n_features = X_train.shape

    n_estimators = [5, 10, 50, 100, 150, 200, 250, 300]
    max_depth = [5, 10, 25, 50, 75, 100]
    min_samples_leaf = [1, 2, 4, 8, 10]
    min_samples_split = [2, 4, 6, 8, 10]
    max_features = ["sqrt", "log2", None]

    hyperparameter = {'n_estimators': n_estimators,
                      'max_depth': max_depth,
                      'min_samples_leaf': min_samples_leaf,
                      'min_samples_split': min_samples_split,
                      'max_features': max_features,
                      }

    base_model_rf = RandomForestClassifier(criterion="gini", random_state=42)
    n_iter_search = 30
    scoring = "accuracy"
    n_selected_features = 10

    # selected feature set, initialized to be empty
    F = []
    count = 0
    ddict = {}
    all_F = []
    all_c = []
    all_acc = []
    all_model = []

    while count < n_selected_features:
        max_acc = 0
        for i in X_train.columns:
            if i not in F:
                F.append(i)
                X_train_tmp = X_train[F]
                acc = 0
                rsearch_cv = RandomizedSearchCV(estimator=base_model_rf,
                                                random_state=42,
                                                param_distributions=hyperparameter,
                                                n_iter=n_iter_search,
                                                cv=5,
                                                scoring=scoring,
                                                n_jobs=-1)
                rsearch_cv.fit(X_train_tmp, y_train)
                best_estimator = rsearch_cv.best_estimator_
                y_pred = best_estimator.predict(X_test[F])
                acc = metrics.accuracy_score(y_test, y_pred)
                F.pop()
                if acc > max_acc:
                    max_acc = acc
                    idx = i
                    best_model = best_estimator

        F.append(idx)
        count += 1

        print("The current number of features: {} - Accuracy: {}%".format(count, round(max_acc * 100, 2)))

        all_F.append(np.array(F))
        all_c.append(count)
        all_acc.append(max_acc)
        all_model.append(best_model)

    c = pd.DataFrame(all_c)
    a = pd.DataFrame(all_acc)
    f = pd.DataFrame(all_F)
    f["All"] = f[f.columns[0:]].apply(
        lambda x: ', '.join(x.dropna().astype(str)), axis=1)

    all_info = pd.concat([c, a, f["All"]], axis=1)
    all_info.columns = ['Num_feature', 'Accuracy', 'Feature']
    all_info = all_info.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)

    all_info.to_csv("subset_accuracy_NR_AB.csv", index=False)

    f.to_csv("feature_subset_NR_AB.csv")

    # Fetching the best hyperparameters
    print(rsearch_cv.best_params_)

    with open('best_params_NR_AB.txt', 'w+') as file:
        file.write(str(rsearch_cv.best_params_))


def main():
    # *** groups ***
    # Read drug-target interactions
    df_inter = pd.read_csv('df_NR_AB.csv')

    df_inter.drop(df_inter.columns[df_inter.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)
    # print(df_inter.head())
    df_final = One_SVM_US(df_inter)

    FFS_RF(df_final)


if __name__ == "__main__":
    main()

# XGBoost Classifiers for drug-target prediction (5-fold)

import pandas as pd
import numpy as np

from xgboost import XGBClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from itertools import cycle


# Balanced with One-SVM-US ############################################################################
df_final = pd.read_csv('df_final_NR_AB.csv')

#######################################################################################################

# Remove unnamed columns
df_final.drop(df_final.columns[df_final.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)

y_final = df_final['label'].values
df_final.drop(['label', 'protein_no', 'drug_no'], axis=1, inplace=True)
#print(df_final.head())

#######################################################################################################
# Read selected feature list
df_features_final = pd.read_csv('subset_accuracy_NR_AB.csv')

#######################################################################################################

features_final = df_features_final.loc[0, 'Feature']
features_final = features_final.replace(" ", "")
features_list1 = list(features_final.split(','))
print(features_list1)
print(len(features_list1))

X_final = df_final[[c for c in df_final.columns if c in features_list1]].values
#######################################################################################################

# Balanced with random undersampling ##################################################################
df_rand = pd.read_csv('df_NR_AB_rand.csv')

#######################################################################################################

# Remove unnamed columns
df_rand.drop(df_rand.columns[df_rand.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)

y_rand = df_rand['label'].values
df_rand.drop(['label', 'protein_no', 'drug_no'], axis=1, inplace=True)
#print(df_rand.head())

#######################################################################################################
# Read selected feature list
df_features_rand = pd.read_csv('subset_accuracy_NR_AB_rand.csv')

#######################################################################################################

features_rand = df_features_rand.loc[0, 'Feature']
features_rand = features_rand.replace(" ", "")
features_list2 = list(features_rand.split(','))
print(features_list2)
print(len(features_list2))

X_rand = df_rand[[c for c in df_rand.columns if c in features_list2]].values
#######################################################################################################

# prepare the cross-validation procedure
cv = KFold(n_splits=5, random_state=1, shuffle=True)


# Set up plotting area
plt.figure()
lw = 2

colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])

## Create a XGBoost Classifier##########################################################################
classifier_xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)

## Training XGBoost Classifier with 1SVM balanced ######################################################

mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)
i = 0
for (train, test), color in zip(cv.split(X_final, y_final), colors):
    probas_ = classifier_xgb.fit(X_final[train], y_final[train]).predict_proba(X_final[test])
    # Compute ROC curve and area the curve
    fpr, tpr, thresholds = roc_curve(y_final[test], probas_[:, 1])
    mean_tpr += np.interp(mean_fpr, fpr, tpr)
    mean_tpr[0] = 0.0
    roc_auc = auc(fpr, tpr)

    i += 1

mean_tpr /= cv.get_n_splits(X_final, y_final)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
mean_auc = format(round(mean_auc, 2), ".2f")
plt.plot(mean_fpr, mean_tpr, label="With One-SVM-US, AUROC="+str(mean_auc))

## Training XGBoost Classifier with Random balanced #####################################################

mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)
i = 0
for (train, test), color in zip(cv.split(X_rand, y_rand), colors):
    probas_ = classifier_xgb.fit(X_rand[train], y_rand[train]).predict_proba(X_rand[test])
    # Compute ROC curve and area the curve
    fpr, tpr, thresholds = roc_curve(y_rand[test], probas_[:, 1])
    mean_tpr += np.interp(mean_fpr, fpr, tpr)
    mean_tpr[0] = 0.0
    roc_auc = auc(fpr, tpr)

    i += 1

mean_tpr /= cv.get_n_splits(X_rand, y_rand)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
mean_auc = format(round(mean_auc, 2), ".2f")
plt.plot(mean_fpr, mean_tpr, label="With Random sampling, AUROC="+str(mean_auc))


#######################################################################################################
plt.plot([0, 1], [0, 1], color="black", lw=lw, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")

#plt.title("Group AB - EN")
#plt.title("Group AB - GPCR")
#plt.title("Group AB - IC")
plt.title("Group AB - NR")

plt.legend(loc="lower right")
plt.show()